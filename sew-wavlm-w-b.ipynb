{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":52324,"databundleVersionId":6229904,"sourceType":"competition"}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n! pip install transformers\n! pip install jiwer\n! pip install --upgrade librosa","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-15T12:27:28.515625Z","iopub.execute_input":"2023-12-15T12:27:28.516256Z","iopub.status.idle":"2023-12-15T12:28:04.550397Z","shell.execute_reply.started":"2023-12-15T12:27:28.516217Z","shell.execute_reply":"2023-12-15T12:28:04.549086Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nfrom kaggle_secrets import UserSecretsClient\nfrom datetime import datetime\n\nimport librosa\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:28:04.552237Z","iopub.execute_input":"2023-12-15T12:28:04.552552Z","iopub.status.idle":"2023-12-15T12:28:10.078324Z","shell.execute_reply.started":"2023-12-15T12:28:04.552524Z","shell.execute_reply":"2023-12-15T12:28:10.077567Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n    Wav2Vec2CTCTokenizer,\n    Wav2Vec2FeatureExtractor\n) ","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:28:10.079402Z","iopub.execute_input":"2023-12-15T12:28:10.079892Z","iopub.status.idle":"2023-12-15T12:28:27.743497Z","shell.execute_reply.started":"2023-12-15T12:28:10.079864Z","shell.execute_reply":"2023-12-15T12:28:27.742453Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"Config = {\n    'audio_dir': '/kaggle/input/bengaliai-speech/train_mp3s',\n    'model_name': 'facebook/wav2vec2-base',\n    'lr': 3e-4,\n    'wd': 1e-5,\n    'T_0': 10,\n    'T_mult': 2,\n    'eta_min': 1e-6,\n    'nb_epochs': 5,\n    'train_bs': 16,\n    'valid_bs': 16,\n    'sampling_rate': 16000,\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:28:27.746185Z","iopub.execute_input":"2023-12-15T12:28:27.746437Z","iopub.status.idle":"2023-12-15T12:28:27.751504Z","shell.execute_reply.started":"2023-12-15T12:28:27.746415Z","shell.execute_reply":"2023-12-15T12:28:27.750502Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def read_audio(mp3_path, target_sr=16000):\n    audio, sr = librosa.load(mp3_path, sr=32000)\n    audio_array = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n    return audio_array\n\ndef construct_vocab(texts):\n    all_text = \" \".join(texts)\n    vocab = list(set(all_text))\n    return vocab\n\n\ndef save_vocab(dataframe):\n    vocab = construct_vocab(dataframe['sentence'].tolist())\n    vocab_dict = {v: k for k, v in enumerate(vocab)}\n    vocab_dict[\"__\"] = vocab_dict[\" \"]\n    _ = vocab_dict.pop(\" \")\n    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n\n    with open('vocab.json', 'w') as fl:\n        json.dump(vocab_dict, fl)\n\n    print(\"Created Vocab file!\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:28:27.752679Z","iopub.execute_input":"2023-12-15T12:28:27.752987Z","iopub.status.idle":"2023-12-15T12:28:27.764572Z","shell.execute_reply.started":"2023-12-15T12:28:27.752962Z","shell.execute_reply":"2023-12-15T12:28:27.763846Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class ASRDataset(Dataset):\n    def __init__(self, df, config, is_test=False):\n        self.df = df\n        self.config = config\n        self.is_test = is_test\n    \n    def __getitem__(self, idx):\n        # First read and pre-process the audio file\n        audio = read_audio(self.df.loc[idx]['path'])\n        audio = processor(\n            audio, \n            sampling_rate=self.config['sampling_rate']\n        ).input_values[0]\n        \n        if self.is_test:\n            return {'audio': audio, 'label': -1}\n        else:\n            # If we are training/validating, also process the labels (actual sentences)\n            with processor.as_target_processor():\n                labels = processor(self.df.loc[idx]['sentence']).input_ids\n            return {'audio': audio, 'label': labels}\n        \n    def __len__(self):\n        return len(self.df)\n    \ndef ctc_data_collator(batch):\n    input_features = [{\"input_values\": sample[\"audio\"]} for sample in batch]\n    label_features = [{\"input_ids\": sample[\"label\"]} for sample in batch]\n    batch = processor.pad(\n        input_features,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    with processor.as_target_processor():\n        labels_batch = processor.pad(\n            label_features,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        \n    labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n    batch[\"labels\"] = labels\n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:28:27.765664Z","iopub.execute_input":"2023-12-15T12:28:27.765981Z","iopub.status.idle":"2023-12-15T12:28:27.778032Z","shell.execute_reply.started":"2023-12-15T12:28:27.765946Z","shell.execute_reply":"2023-12-15T12:28:27.777215Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, device='cuda:0'):\n    model.train()\n    pbar = tqdm(train_loader, total=len(train_loader))\n    avg_loss = 0\n    for data in pbar:\n        data = {k: v.to(device) for k, v in data.items()}\n        loss = model(**data).loss\n        loss_itm = loss.item()\n        \n        avg_loss += loss_itm\n        pbar.set_description(f\"loss: {loss_itm:.4f}\")\n        \n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        \n    return avg_loss / len(train_loader)\n\n@torch.no_grad()\ndef valid_one_epoch(model, valid_loader, device='cuda:0'):\n    pbar = tqdm(valid_loader, total=len(valid_loader))\n    avg_loss = 0\n    for data in pbar:\n        data = {k: v.to(device) for k, v in data.items()}\n        loss = model(**data).loss\n        loss_itm = loss.item()\n        \n        avg_loss += loss_itm\n        pbar.set_description(f\"val_loss: {loss_itm:.4f}\")\n\n    return avg_loss / len(valid_loader)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:28:27.778907Z","iopub.execute_input":"2023-12-15T12:28:27.779152Z","iopub.status.idle":"2023-12-15T12:28:27.797006Z","shell.execute_reply.started":"2023-12-15T12:28:27.779129Z","shell.execute_reply":"2023-12-15T12:28:27.796327Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import SEWForCTC\ndf = pd.read_csv(\"/kaggle/input/bengaliai-speech/train.csv\")\n\n# Get a paths feature for reading in during dataloading\ndf['path'] = df['id'].apply(lambda x: os.path.join(Config['audio_dir'], x+'.mp3'))\ntrain_df = df[df['split'] == 'train'].sample(frac=.001).reset_index(drop=True)\nvalid_df = df[df['split'] == 'valid'].sample(frac=.001).reset_index(drop=True)\nprint(f\"Training on samples: {len(train_df)}, Validation on samples: {len(valid_df)}\")\n\n# Construct and save the vocab file\nsave_vocab(df)\n\n# Init the tokenizer, feature_extractor, processor and model\ntokenizer = Wav2Vec2CTCTokenizer(\n    \"./vocab.json\", \n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    word_delimiter_token=\"__\"\n)\nfeature_extractor = Wav2Vec2FeatureExtractor(\n    feature_size=1, \n    sampling_rate=Config['sampling_rate'], \n    padding_value=0.0, \n    do_normalize=True, \n    return_attention_mask=False\n)\nprocessor = Wav2Vec2Processor(\n    feature_extractor=feature_extractor, \n    tokenizer=tokenizer\n)\n\nmodel = SEWForCTC.from_pretrained(\"asapp/sew-tiny-100k-ft-ls100h\",\n    ctc_loss_reduction=\"mean\", \n    ignore_mismatched_sizes=True,\n    pad_token_id=processor.tokenizer.pad_token_id,\n    vocab_size = len(tokenizer),\n)\n\n# Freeze the feature encoder part since we won't be training it\nmodel.to('cuda')\nmodel.freeze_feature_encoder()\noptimizer = torch.optim.AdamW(\n    model.parameters(), \n    lr=Config['lr'], \n    weight_decay=Config['wd']\n)\n\n# Construct training and validation dataloaders\ntrain_ds = ASRDataset(train_df, Config)\nvalid_ds = ASRDataset(valid_df, Config)\n\ntrain_loader = DataLoader(\n    train_ds, \n    batch_size=Config['train_bs'], \n    collate_fn=ctc_data_collator, \n)\nvalid_loader = DataLoader(\n    valid_ds,\n    batch_size=Config['valid_bs'],\n    collate_fn=ctc_data_collator,\n)\n\n# Train the model\nbest_loss = float('inf')\nfor epoch in range(Config['nb_epochs']):\n    print(f\"{'='*40} Epoch: {epoch+1} / {Config['nb_epochs']} {'='*40}\")\n    train_loss = train_one_epoch(model, train_loader, optimizer)\n    valid_loss = valid_one_epoch(model, valid_loader)\n    print(f\"train_loss: {train_loss:.4f}, valid_loss: {valid_loss:.4f}\")\n\n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), f\"sew_base_bengaliAI.pt\")\n        print(f\"Saved the best model so far with val_loss: {valid_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:28:27.798235Z","iopub.execute_input":"2023-12-15T12:28:27.798473Z","iopub.status.idle":"2023-12-15T12:31:20.219023Z","shell.execute_reply.started":"2023-12-15T12:28:27.798452Z","shell.execute_reply":"2023-12-15T12:31:20.218025Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training on samples: 934, Validation on samples: 30\nCreated Vocab file!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4526df6923b453fadcf3eebe49aa5eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/163M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1601681c69845c8a1502d3337b2a934"}},"metadata":{}},{"name":"stderr","text":"Some weights of SEWForCTC were not initialized from the model checkpoint at asapp/sew-tiny-100k-ft-ls100h and are newly initialized because the shapes did not match:\n- lm_head.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([89]) in the model instantiated\n- lm_head.weight: found shape torch.Size([32, 512]) in the checkpoint and torch.Size([89, 512]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"======================================== Epoch: 1 / 5 ========================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/59 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098d40bbb9d44496b46701feb18be814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3d7a3820c142eca2abb6d15475dcf0"}},"metadata":{}},{"name":"stdout","text":"train_loss: 5.7335, valid_loss: 4.8140\nSaved the best model so far with val_loss: 4.8140\n======================================== Epoch: 2 / 5 ========================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/59 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8b37949b69d4ffeb2a3bd7f0a56ad4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff72468c00ab4fd4b7e18cd68aec9620"}},"metadata":{}},{"name":"stdout","text":"train_loss: 4.8638, valid_loss: 4.7757\nSaved the best model so far with val_loss: 4.7757\n======================================== Epoch: 3 / 5 ========================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/59 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3d350ce12b44edbb73f3879aa3fed47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cc079c75e534aa38b81e6dfa5d52aea"}},"metadata":{}},{"name":"stdout","text":"train_loss: 4.8257, valid_loss: 4.7656\nSaved the best model so far with val_loss: 4.7656\n======================================== Epoch: 4 / 5 ========================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/59 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a178830252f14b8abe66c20173b116dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e08f4fbb0b7c4c2489116903702e5cf3"}},"metadata":{}},{"name":"stdout","text":"train_loss: 4.9194, valid_loss: 4.8257\n======================================== Epoch: 5 / 5 ========================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/59 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b626b4f18cfc499bbf79ad516eb8f3c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a62cfa91e26043f5ba9e6f818e6d1b78"}},"metadata":{}},{"name":"stdout","text":"train_loss: 4.8222, valid_loss: 4.7693\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}